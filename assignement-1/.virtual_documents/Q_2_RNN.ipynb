import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences



# Sample text data (small corpus)
corpus = [
    "I like deep learning",
    "Deep learning is powerful",
    "I use TensorFlow for ML projects",
    "TensorFlow is great for deep learning",
    "RNNs are good for sequential data"
]

# Tokenize the text
tokenizer = Tokenizer()
tokenizer.fit_on_texts(corpus)

# Convert text to sequences
sequences = []
for sentence in corpus:
    token_list = tokenizer.texts_to_sequences([sentence])[0]
    for i in range(1, len(token_list)):
        sequences.append(token_list[:i+1])  # Creating input-output pairs

# Pad sequences to make them of equal length
max_sequence_length = max(len(seq) for seq in sequences)
sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='pre')

# Split into input (X) and output (y)
X, y = sequences[:, :-1], sequences[:, -1]

# Convert output labels to categorical
vocab_size = len(tokenizer.word_index) + 1
y = tf.keras.utils.to_categorical(y, num_classes=vocab_size)

print("Vocabulary Size:", vocab_size)
print("Max Sequence Length:", max_sequence_length)



# Define the RNN model
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(vocab_size, 10, input_length=max_sequence_length - 1),  # Embedding layer
    tf.keras.layers.SimpleRNN(64, return_sequences=True),
    tf.keras.layers.SimpleRNN(64),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(vocab_size, activation='softmax')  # Output layer for word prediction
])

# Compile the model
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
history = model.fit(X, y, epochs=25, verbose=1)

model.save('./saved_model/Q_2.keras')


plt.plot(history.history['accuracy'])
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Training Accuracy Over Time')
plt.show()



def predict_next_word(model, tokenizer, text, max_sequence_length):
    sequence = tokenizer.texts_to_sequences([text])[0]
    sequence = pad_sequences([sequence], maxlen=max_sequence_length-1, padding='pre')
    
    predicted_index = np.argmax(model.predict(sequence), axis=-1)[0]
    for word, index in tokenizer.word_index.items():
        if index == predicted_index:
            return word
    return None

# Test the model
input_text = "I like"
predicted_word = predict_next_word(model, tokenizer, input_text, max_sequence_length)
print(f"Next word prediction: {predicted_word}")
print(f"==================== \n {input_text} {predicted_word}")




